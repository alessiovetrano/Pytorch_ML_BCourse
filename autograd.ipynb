{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch beginner course: Autograd (Pytorch for gradient computation)\n",
    "\n",
    "## Summary\n",
    "\n",
    "- [What is Autograd?](#what-is-autograd)\n",
    "- [How machines compute derivates](#how-machines-compute-derivates)\n",
    "- [Gradient calculation](#gradient-calculation)\n",
    "- [Backward function and Jacobian matrix](#backward-function-and-jacobian-matrix)\n",
    "- [Gradient history management](#gradient-history-management)\n",
    "- [Glossary of the used tools](#glossary-of-the-used-tools)\n",
    "    - [Methods](#methods)\n",
    "    - [Properties](#properties)\n",
    "- [References](#references)\n",
    "- [Author](#author)\n",
    "\n",
    "## What is autograd?\n",
    "\n",
    "Machine learning can be considered as an optimization problem, usually a minimization problem, for this reason the **derivates** are the main tools used by the training algorithms, thankfully pytorch offers a specifi module to solve the *derivates* and in particular the *gradients* of the functions, this module is `torch.autograd`.\n",
    "\n",
    "As we will see, the concept of the derivates is very important to understand the mechanism of a learning algorithms, and with `autograd` we will not waste time to implements manually all functions that calculate the gradients, pytorch is one of the most used deep learning framework also for the `autograd`.\n",
    "\n",
    "The main pros of the `autograd` are:\n",
    "1. *Easy to use*\n",
    "2. *Efficiency*\n",
    "3. *Flexibility*\n",
    "\n",
    "## How machines compute derivates\n",
    "\n",
    "In computer science we have three different ways to calculate derivates:\n",
    "\n",
    "* Numerical differentiation\n",
    "* Symbolic differentiation\n",
    "* Automatic differentiation *(that combine the two previous approaches)*\n",
    "\n",
    "in this lecture we will not study the theory behind this three approaches, but that's important to know the main pros and cons among them:\n",
    "\n",
    "| Method | Precision | Velocity | Application |\n",
    "|:------:|:---------:|:--------:|:-----------:|\n",
    "| Numerical Differentiation | Approximate | Rapidly | Simple function |\n",
    "| Symbolic Differentiation | Exactly | Slow | Complex function |\n",
    "| Automatic Differentiation | Exactly or Approximate | Rapidly | Complex function |\n",
    "\n",
    "The `autograd` module use the **Automatic differentiation**, which combines the **Numerical** and **Symbolic** approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient calculation\n",
    "\n",
    "Now we will see in practice how is possible to calculate the gradients with `autograd`.\n",
    "\n",
    "For the first is necessary to set the attribute `requires_grad=True` to specify that we want track the tensor during the gradients calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([4., 2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.,3.], requires_grad=True)\n",
    "y = torch.tensor([4.,2.], requires_grad=True)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our tensors are ready to know their gradients during the computation.\n",
    "\n",
    "For semplicity now we apply a simple operation between this two tensors and we will save the output of this opertation into an another tensor `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24., 31.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = (x**3)+(y**2)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is very importanto to pay attention to the last output, we will see that the our tensor `z` have a particular attribute named `grad_fn` that explain the gradient function that we will use to apply the **backward** method, as we can see the gradient function is named `<AddBackward0>` and this information is very important for us because the name of gradient function explain us that the tensor `z` was born from a sum operation *(the sum between $x^3$ and $y^2$)*.\n",
    "\n",
    "The name of *gradient function* change if we change the basic operation to obtain the output tensor, as follow we show some of these functions:\n",
    "\n",
    "| Operation | Gradient Function |\n",
    "|:---------:|:-----------------:|\n",
    "| `+` | `<AddBackward>` |\n",
    "| `-` | `<SubBackward>` |\n",
    "| `*` | `<MulBackward>` |\n",
    "| `/` | `<DivBackward>` |\n",
    "| `mean()` | `<MeanBackward>` |\n",
    "\n",
    "Essentially, the `grad_fn` contain an object instance pointer of the class `torch.autograd.Function` if the tensor was made by an operation between two tensor, otherwise the `grad_fn` attribute have `None` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn of z:  <AddBackward0 object at 0x000001B454F939A0>\n",
      "grad_fn of x:  None\n",
      "grad_fn of y:  None\n"
     ]
    }
   ],
   "source": [
    "print(\"grad_fn of z: \", z.grad_fn)   # z was made by an operation between x and y\n",
    "print(\"grad_fn of x: \", x.grad_fn)   # x was made by a user (don't have a gradient function)\n",
    "print(\"grad_fn of y: \", y.grad_fn)   # y was made by a user (don't have a gradient function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can procede to compute a gradients of `x` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of x: tensor([12., 27.])\n",
      "Gradients of y: tensor([8., 4.])\n"
     ]
    }
   ],
   "source": [
    "# backward() method compute the gradients respect to the leaf of the graph (x and y are the leafs in our scenario)\n",
    "# the backward() method work only on a scalar value, for this reason we must do an operation that compress the output tensor in a scalar value,\n",
    "# in this case we have used a sum() function, but we could have used any function, for example mean()\n",
    "z = z.sum()\n",
    "\n",
    "# Now we can calculate the gradients...\n",
    "z.backward()\n",
    "\n",
    "# ...and print the gradients of our input tensor x and y\n",
    "print(\"Gradients of x:\", x.grad)\n",
    "\n",
    "print(\"Gradients of y:\", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output obtained is given by the calculation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial z}{\\partial x}\n",
    "\\\\\n",
    "\\frac{\\partial z}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3x^2\n",
    "\\\\\n",
    "2y\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3x_1^2, 3x_2^2\n",
    "\\\\\n",
    "2y_1, 2y_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3*2^2, 3*3^2\n",
    "\\\\\n",
    "2*4, 2*2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "12, 27\n",
    "\\\\\n",
    "8, 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Below we can see the representation of our simple computation\n",
    "\n",
    "![backward concept](images/backward.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward function and Jacobian matrix\n",
    "\n",
    "parlare della funzione backward di quando prende in input un vettore, di come funziona sotto la funzione backward che sostanzialmente si basa su un prodotto Jacobiana per vettore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient history management\n",
    "\n",
    "Mettere tutti i metodi e le procedure per cancellare il tracciamento del gradiente come with torch.no_grad() e ricordare che non tenere traccia snellisce di molto i costi computazionali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of the used tools\n",
    "\n",
    "### Methods\n",
    "\n",
    "- `torch.autograd.backward()`\n",
    "\n",
    "### Properties\n",
    "\n",
    "- `torch.autograd.grad`\n",
    "- `torch.autograd.requires_grad`\n",
    "- `torch.autograd.grad_fn`\n",
    "\n",
    "## References\n",
    "\n",
    "[Pytorch documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "## Author\n",
    "\n",
    "Emilio Garzia, 2024\n",
    "\n",
    "[Github](https://github.com/EmilioGarzia)\n",
    "\n",
    "[Linkedin](https://www.linkedin.com/in/emilio-garzia-58a934294/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
